=== INPUT ===
slides: 
video : E:\video_note_agent\example\cs336_02.mp4
query : 


=== TRANSCRIPT ===
Channel 0:
Okay, so last lecture I gave an overview of language models and what it means to build them from scratch and why we want to do that. I also talked about tokenization, which is going to be the first half of the first assignment. Today's lecture will be going through actually building a model. UM, WE'LL DISCUSS THE PRIMITIVES IN PYTORCH THAT ARE NEEDED. UM, WE'RE GOING TO START WITH TENSORS, BUILD MODELS, OPTIMIZERS AND TRAINING LOOP, AND WE'RE GONNA PLACE CLOSE ATTENTION TO EFFICIENCY, IN PARTICULAR HOW WE'RE USING RESOURCES, BOTH UM MEMORY AND COMPUTE. Okay, so to motivate things a bit, here's some questions. Okay, these questions are going to be answerable by napkin math, so get your napkins out. So how long would it take to train a 70 billion parameter dense transformer model on 15 trillion tokens on 1,024 H100s? Okay, so I'm just going to sketch out the sort of the give you a flavor of the type of things that we want to do. Okay, so here's how you go about reasoning it. You count the total number of flops needed to train, so that's six times the number of parameters times the number of tokens. Okay? And where does that come from? That will be what we'll talk about in this lecture. You can look at the promise on the number of flops per second that H100 gives you. UM, THE MFU, WHICH IS SOMETHING WE'LL SEE LATER. LET'S JUST SET IT TO POINT FIVE. UM, AND YOU CAN LOOK AT THE NUMBER OF FLOPS PER DAY THAT YOUR HARDWARE IS GOING TO GIVE YOU AT THIS PARTICULAR MFU. SO A THOUSAND TWENTY-FOUR OF THEM. UM. For one day, and then you just divide the total number of flops you need to train the model by the number of flops that you're supposed to get, okay? And that gives you about 144. Okay, so this is very simple calculations at the end of the day. We're going to go through a bit more where these numbers come from, and in particular where the six times number of parameters times number of tokens comes from. UM, okay, so here's the question: what is the largest model you can train on H eight H one hundred using Adam W if you're not being too clever? Okay, so H one hundred has eighty gigabytes of HBM memory. The number of bytes per parameter that you need for the parameters, the gradients optimizer state is sixteen, and we'll talk more about where that comes from. And the number of parameters is basically total amount of memory divided by number of bytes you need per parameter. And that gives you about 40 billion parameters, okay? And this is very rough because it doesn't take you into activations, which depends on batch size and sequence length, which I'm not really going to talk about, but will be important for assignment one. Okay, so this is a rough back of the calculation, and this is something that you're probably not used to doing. You just implement a model, you train it, and what happens happens. But remember that efficiency is the name of the game, and to be efficient, you have to know exactly how many flops you're actually expending, because when these numbers get large, these directly translate into dollars, and you want that to be as small as possible. Okay, so we'll talk more about the details of how these numbers arise. We will not actually go over the transformer, so Tatsu is going to talk over the conceptual overview of that next time. And there's many ways you can learn about a transformer if you haven't already looked at it. There's assignment one, if you do assignment one, you'll definitely know what a transformer is. And the handout actually does a pretty good job of walking through all the different pieces. There's a mathematical description, if you like pictures, there's pictures. There's a lot of stuff you can look on online. But instead, I'm going to work with simpler models and really talk about the primitives and the resource accounting piece. Okay, so remember last time I said what kinds of knowledge can you learn? So mechanics in this lecture it's going to be just PyTorch and understanding how PyTorch works at a fairly primitive level. So that's will be pretty straightforward. Mindset is about resource accounting and it's not hard, it's just you just have to do it. And intuitions unfortunately this is just going to be broad strokes for now. Actually there's not really much intuition that I'm going to talk about in terms of how anything we're doing translates to good models. This is more about the mechanics and mindset. Okay, so let's start with memory accounting, and then I'll talk about compute accounting, and then we'll build up bottom up. Okay, so the best place to start is a tensor. So tensors are the building block for storing everything in deep learning: parameters, gradients, optimizers, state, data, activations. There's sort of these atoms. UM, YOU CAN READ LOTS OF DOCUMENTATION ABOUT THEM. UM, YOU'RE PROBABLY VERY FAMILIAR WITH HOW TO CREATE TENSORS. THERE'S CREATING TENSORS DIFFERENT WAYS. UM, YOU CAN ALSO CREATE A TENSOR AND NOT INITIALIZE IT AND USE SOME SPECIAL UH INITIALIZATION UM FOR THE PARAMETERS UM TO. If you want, okay? So those are tensors. So let's talk about memory and how much memory tensors take up. So every tensor that will probably be interested in is sort of a floating point number, and so there's many ways to represent floating point. So the most default way is float 32. And float 2 has 3 2 bits. They are allocated one for sign, eight for exponent, and 2 3 for the fraction. So exponent gives you dynamic range and fraction gives you different basically specifies different values. So float 3 2 is also known as fp 3 2 or single precision. Is sort of the gold standard in computing. Some people also refer to float 3 2 as full precision. That is a little bit confusing because full is really depending on who you are talking to. If you are talking to a scientific computing person, they will kind of laugh at you when you say float 3 2 is really full because they will use float 6 4 or even more. But if you are talking to a machine learning person, float 3 2 is the max you ever probably need to go. Because deep learning is kind of sloppy like that. Okay, so let's look at the memory. So the memory is it's very simple. It's determined by the number of values you have in your tensor and the data type of each value. Okay, so if you create a torch tensor of a four by eight matrix, the default it will give you a type of float thirty two. The size is four by eight and the number of elements is thirty two. Each element size is four bytes, thirty-two bits is four bytes, and the memory usage is simply the number of elements times the number of size of each element, and that will give you one hundred and twenty-eight bytes. Okay, so this should be pretty easy. And just to give some intuition, if you get the one matrix in the F four layer of GPT three is this number by this number, and that gives you two point three gigabytes. Okay, so that's one matrix is, you know, can these matrices can be pretty big. Okay, so float thirty two is a default, but of course these matrices get big, so naturally you want to make them smaller, so use less memory, and also it turns out if you make them smaller, you also make it go faster too. Okay, so another. Representation is called float 16, and as the name suggests, it's 16 bits where both exponent and fraction are shrunk down from eight to five and 23 to ten. Okay, so this is known as half precision, and it cuts down half the memory. That's all great, except for the dynamic range for these float sixteen isn't great. So for example, if you try to make a number like ten e one e minus eight in float sixteen, it basically rounds down to zero and you get underflow. Okay, so the flow sixteen is not great for representing very small numbers or very big numbers. As a matter of fact, so if you use flow sixteen for training for small models, it's probably going to be okay, but for large models when you're having lots of matrices and you can get instability or underflow or overflow and bad things happen. Okay, so one thing that has happened, which is nice, is there's been another representation of B float sixteen, which stands for brain float. This was developed in twenty eighteen to address the issue that for deep learning, we actually care about dynamic range more than we care about this fraction. So basically. BF sixteen allocates more to the exponent and less to the fraction. Okay, so it uses the same memory as floating point sixteen, but it has a dynamic range of float thirty two. Okay, so that sounds really good, and it actually the catch is that this resolution which is determined by the fraction is worse, but this doesn't matter as much for deep learning. So now if you try to create a tensor with one e minus eight and bf sixteen, then you get something that's not zero. Okay, so you can dive into the details. I am not going to go into this, but you can stare out to the actual full specs of all the different floating point operations. Okay, so BF 1 6 is basically what you will typically use to do computations because it is sort of good enough for few forward computations. It turns out that for storing optimizer states and parameters, you still need float 3 2. For otherwise your training will go haywire. So if you're bold, so now we have something called FP eight or eight bit. And as the name suggests, this was developed in twenty twenty two. By NVIDIA, so now they have essentially. If you look at FP and BF-16, it's like this. And FP... Well, you really don't have that many bits to store stuff, right? So it's very crude. There's two sort of variants depending on if you want to have more resolution or more dynamic range. And I'm not going to say too much about this, but FP eight is supported by H one hundred. It's not really available on previous generation. But at a high level, training with flow thirty two, which is I think is what you would do if you're not trying to optimize too much. And it's sort of safe. It requires more memory. You can go down to FP eight or BF sixteen. UM, AND BUT YOU CAN GET SOME INSTABILITY. UM, BASICALLY I DON'T THINK YOU WOULD PROBABLY WANT TO USE A FLOAT SIXTEEN AT THIS POINT FOR FOR DEEP LEARNING. And you can become more sophisticated by looking at particular places in your pipeline, either for forward pass or backward pass or optimizers or gradient accumulation, and really figure out what the minimum precision you need at this particular places, and that's called gets into kind of mixed precision training. So for example, some people like to use flow 32 for the. The attention to make sure that doesn't kind of get messed up, but for simple feed forward passes with mammals BF sixteen is fine. Okay, pause a bit for questions. So we talked about tensors and we looked at depending on how what representation, how much storage they take. Yeah, so the question is when would you use float 32 or BF16? I don't have time to get into the exact details, and it sort of varies depending on the model size and everything. But generally, for the parameters and optimizer states, you use float 32. You can think about BF16 as something that's more transitory. Like you basically take your parameters, you cast it to BF sixteen, and you kind of run ahead with that model. But then the thing that you're going to accumulate over time, you want to have higher precision. いや。Okay, so now let's talk about compute. So that was memory. So compute obviously depends on what the hardware is. By default, tensors are stored in CPU. So for example, if you just in PyTorch say x equals torch.zeros(32, 32), then it will put it on your CPU. It will be in. The CPU memory, of course, that's no good because if you're not using your GPU, then you're going to be orders of magnitude too slow. So you need to explicitly say in PyTorch that you need to move it to the GPU. And this is actually just to make it very clear in pictures. There's a CPU, it has RAM, and that has to be moved over to the GPU. There's a data transfer which is cut, which takes some work, takes some time. So whenever you have a tensor in PyTorch, you should always keep in your mind where is this residing, because just looking at the variable or just looking at the code, you can't always tell. AND IF YOU WANT TO BE CAREFUL ABOUT COMPUTATION AND DATA MOVEMENT, YOU HAVE TO REALLY KNOW WHERE IT IS. UM, YOU CAN PROBABLY DO THINGS LIKE ASSERT UM WHERE IT IS IN VARIOUS PLACES OF CODE JUST TO UM DOCUMENT OR BE SURE. Okay, so let's look at what hardware we have. So we have in this case we have one GPU. This was run on the H one hundred clusters that you guys have access to, and this GPU is H one hundred eighty gigabytes of high bandwidth memory. And there's a, you know, it gives you the cache size and so on. Okay，so。So if you have remember the X is on CPU, you can move it just by specifying two which is a kind of a general PyTorch function. You can also create a tensor directly on a GPU so you don't have to move it at all. If everything goes well, I'm looking at the memory allocated before and after. The difference should be exactly two thirty by thirty two by thirty two matrices of four byte floats. Okay, so it's a one nine two. Okay, so this is a sanity check that the code is doing what is advertised. Okay, so now you have your tensors on the GPU. What do you do? So there's many operations that you'll be needing for assignment one and in general to do any deep learning application. And most tensors you just create by performing operations on other tensors, and each operation has some memory and compute footprint. So let's make sure we understand that. So first of all, what is actually a tensor in PyTorch? Tensors are a mathematical object in PyTorch. They're actually pointers into some allocated memory. Okay, so if you have, let's say, a matrix four by four matrix, what it actually looks like is a long array. And what the tensor has is metadata that specifies how to get to address into that array, and the metadata is going to be two numbers, a stride for each or actually number of one number per dimension of the tensor. In this case, because there is two dimensions, it is stride 0 and stride one. STRIDES ZERO UH SPECIFIES IF YOU WERE IN DIMENSION ZERO TO GET TO THE NEXT ROW, TO INCREMENT THAT INDEX. HOW MANY DO YOU HAVE TO SKIP? AND SO GOING DOWN THE ROWS, YOU SKIP FOUR. SO STRIDE UH ZERO IS FOUR AND UM TO GO TO THE NEXT COLUMN, UM YOU SKIP ONE. SO STRIDE UH ONE IS ONE. Okay, so with that, to find an element, let's say one two one comma two, it's simply just multiply the indexes by the stride and you get to your index which is six here. So that would be here or here. Okay, so that's basically what's going underneath the hood for tensors. Okay, so this is relevant because you can have multiple tensors that use the same storage, and this is useful because you don't want to copy the tensor all over the place. So imagine you have a two by three matrix here. Many operations don't actually create a new tensor; they just create a different view and doesn't make a copy. So you have to make sure that. UM, YOUR MUTATIONS, UH, IF YOU START MUTATING ONE TENSOR, IT'S GONNA CAUSE OTHER ONE TO MUTATE. OKAY? SO, UM, FOR EXAMPLE, IF YOU JUST GET, UM, ROW ZERO. Okay, so remember Y is this tensor, and sorry X is one two three four five six, and Y is X zero which is just the first row. Okay, and you can sort of double check. There's this function in road that says if you look at the underlying storage whether these two tensors have the same storage or not. Okay, so this definitely doesn't copy the tensor, it just creates a view. You can get column one. This also doesn't copy the tensor. Oops, don't need to do that. You can call a view function which can take any tensor and look at it in terms of different dimensions. Two by three. Actually, this should be maybe the other way around as a three by two tensor. So that also doesn't change doing copying, you can transpose that also doesn't copy, and then like I said, if you start mutating X, then Y actually gets mutated as well because X and Y are just pointers into the same underlying storage. Okay, so things are one thing that you have to be careful of is that some views are contiguous, which means that if you run through the tensor, it's like just slide going through this array in your storage, but some are not. So in particular, if you transpose it, now you're... you know what does it mean when you're transposing it? You're sort of going down now, so you're kind of... if you imagine going through the tensor, you're kind of skipping around. AND IF YOU HAVE A NON CONTIGUOUS TENSOR, UM, THEN IF YOU TRY TO FURTHER VIEW IT IN A DIFFERENT WAY, THEN THIS IS NOT GONNA WORK. Okay, so in some cases, if you have a non-contiguous tensor, you can make it contiguous first, and then you can apply whatever viewing operation you want to it. And then in this case, X and Y do not have the same storage because contiguous in this case makes a copy. Okay, so this is just ways of slicing and dicing a tensor. Views are free, so feel free to use them. Define different variables to make it sort of easier to read your code because they're not allocating any memory. But remember that contiguous or reshape, which is basically contiguous view. UM, CAN CREATE A COPY AND SO JUST BE CAREFUL WHAT YOU'RE DOING. Okay, questions before moving on. All right, so hopefully a lot of this will be reviewed for those of you who have done a lot of PyTorch before, but it's helpful to just do it systematically, make sure we're on the same page. So here are some operations that do create new tensors, and in particular element-wise operations all create new tensors, obviously because you need it somewhere else to store the new value. There's a triangular U is also an element operation that comes in handy when you want to create a causal attention mask, which you'll need for your assignment. But nothing is interesting that interesting here. Okay, so let's talk about mammals. So the bread and butter of deep learning is matrix multiplications, and I'm sure all of you have done a matrix multiplication, but just in case this is what it looks like: you take a sixteen by thirty-two times a thirty-two by two matrix, you get a sixteen by two matrix. But in general, when we do our machine learning application, all operations are you want to do in a batch. And in the case of language models, this usually means for every example in a batch and for every sequence in a batch, you want to do something. Okay, so generally what you're going to have instead of just a matrix is you're going to have a tensor where the dimensions are typically batch sequence and then whatever thing you're trying to do. In this case, it's a matrix for every token in your data set. And so PyTorch is nice enough to make this work well for you. So when you take this for. UM, DIMENSION TENSOR AND THIS MATRIX. UM, WHAT ACTUALLY ENDS UP HAPPENING IS THAT FOR EVERY BATCH, EVERY EXAMPLE AND EVERY TOKEN, YOU'RE MULTIPLYING THESE TWO MATRICES. OKAY, AND THEN THE RESULT IS THAT YOU GET YOUR YOUR RESULTING MATRIX FOR EACH OF THE FIRST TWO ELEMENTS. SO THIS IS JUST LIKE THERE'S NOTHING FANCY GOING ON, BUT THIS IS JUST A PATTERN THAT UM, I THINK YOU. IS HELPFUL TO THINK ABOUT. Okay, so I am going to take a little bit of a digression and talk about inops. And so the motivation for inops is the following: so normally you in PyTorch you define some tensors and then you see stuff like this where you take x and multiply by y transpose minus 2 minus 1. And you kind of look at this and you say, okay, what is minus two? Well, I think that's this sequence. And then minus one is this hidden because you're indexing backwards. And it's really easy to mess this up because if you look at your code and you see minus one, minus two, you're kind of, if you're good, you write a bunch of comments, but then the comments are can get out of date with the code and then you have a bad time debugging. So the solution is to use inops here. So this is inspired by Einstein's summation notation, and the idea is that we're just going to name all the dimensions instead of relying on indices essentially. Okay, so there's a library called Jax Typing which is helpful for as a way to specify the dimensions in the types. So normally in PyTorch you would just define, write your code, and then you would comment, oh here's what the dimensions would be. So if you use Jax Typing then you have this notation where as a string you just write down what the dimensions are. So this is a slightly kind of more natural way of documenting. Now notice that there's no enforcement here, right? Because PyTorch types are sort of a little bit of a lie in PyTorch. So you can use a checker, right? Yeah, you can raise a check, but not by default. UM. Okay, so let's look at the einsum. So einsum is basically matrix multiplication on steroids with good bookkeeping. So here's our example: here we have x, which is... let's just think about this as you have a batch dimension, you have a sequence dimension, and you have four hidden and y is the same size. You originally had to do this thing. And now what you do instead is you basically write down the dimensions, names of the dimensions of the two tensors, so batch sequence one hidden, batch sequence two hidden, and you just write what you dimensions should appear in the output. Okay, so I write batch here because I just want to basically carry that over, and then I write seek one and seek two, and notice that I don't write hidden, and any dimension that is not named in output is just summed over, and any dimension that is named is sort of just iterated over. Okay, so once you get used to this, this is actually very, very helpful. Maybe it looks if you've seen this for the first time, it might seem a bit strange and long, but trust me, once you get used to it, it'll be better than doing minus two minus one. If you're a little bit slicker, you can use dot dot dot to represent broadcasting over any number of dimensions. So in this case, instead of writing batch, I can just write dot dot dot, and this would handle the case where instead of maybe batch, I have batch one, batch two, or some other arbitrary long sequence. Yeah, question. Does this like is it guaranteed to compile to something efficient? I guess so. The question is, is it guaranteed to compile to something efficient? This I think the short answer is yes. I don't know if you have any nuances. We'll figure out the best way to reduce, the best order of dimensions to reduce. And then use that, if you're using within torch compile, only do that one time, and then reuse the same implementation over and over again. It's gonna be better than anything designed by. OKAY. So, so let's look at reduce. So reduce operates on one tensor and it basically aggregates some dimension or dimensions of the tensor. So you have this tensor before you would write mean to sum over the final dimension, and now you basically say actually okay, so this replaces with sum. So reduce and again you say hidden and hidden is disappeared, so which means that you are aggregating over that dimension. Okay, so you can check that this indeed kind of works over here. Okay, so maybe one final example of this is sometimes in a tensor one dimension actually represents multiple dimensions, and you want to unpack that and operate over one of them and pack it back. So in this case, let's say you have batch sequence, and then this eight-dimensional vector is actually a flattened representation of. Number of heads times some hidden dimension, okay? So and then you have a way of vector that needs to operate on that hidden dimension. So you can do this very elegantly using inops by calling rearrange. And this basically you can think about it. We saw view before, it's kind of like. Kind of a fancy version, which basically looks at the same data but differently. So here it basically says this dimension is actually heads and hidden one. I'm going to explode that into two dimensions, and you have to specify the number of heads here because there's multiple ways to split a number into two. Let's see, this might be a little bit long. Okay, maybe it's not worth looking at right now. And given that X, you can perform. Your transformation using on sum, so this is something hidden one which corresponds to x, and then hidden one hidden two which corresponds to w, and that gives you something hidden two. Okay。UM, AND THEN YOU CAN REARRANGE BACK. SO THIS IS JUST THE INVERSE OF BREAKING UP. SO YOU HAVE YOUR TWO DIMENSIONS AND YOU GROUP IT INTO ONE. SO THAT'S JUST A FLATTENING OPERATION. THAT'S UM... YOU KNOW WITH EVERYTHING, ALL THE OTHER DIMENSIONS KIND OF LEFT ALONE. Okay, so there is a tutorial for this that I would recommend you go through, and it gives you a bit more. So you don't have to use this because you're building it from scratch, so you can kind of do anything you want. But in the assignment one, we do give you guidance, and it's something probably to invest in. ＯＫ。UM, SO NOW LET'S TALK ABOUT UH. COMPUTATION, UH, NO COST OF TENSOR OPERATIONS. SO WE INTRODUCE A BUNCH OF OPERATIONS, UM, AND YOU KNOW HOW MUCH DO THEY COST? SO A FLOATING POINT OPERATION IS, UH, ANY OPERATION FLOATING POINT LIKE ADDITION OR MULTIPLICATION. THESE ARE THEM AND, UM. These are kind of the main ones that are going to, I think, matter in terms of flop count. One thing that is sort of a pet peeve of mine is that when you say flops, it's actually unclear what you mean. So you could mean flops with a lowercase S, which stands for number of floating operations. This measures amount of computation that you've done. OR YOU COULD MEAN FLOPS, UM, ALSO WRITTEN WITH A UPPERCASE S, WHICH MEANS FLOATING POINTS PER SECOND, WHICH IS USED TO MEASURE THE SPEED OF HARDWARE. SO UM, WE'RE NOT GONNA IN THIS CLASS USE UPPERCASE S BECAUSE I FIND THAT VERY CONFUSING, AND JUST WRITE SLASH S TO DENOTE THAT THIS IS FLOATING POINT PER SECOND. Okay. Okay, so just to give you some intuition about flops, GPT-3 took about 323 flops, GPT-4 was 2E25 flops speculation, and there was an US executive order that any foundation model with over 1E26 flops had to be reported to government, which now has been revoked. But the EU as still they're going still has something that hasn't the EU AI Act which is one E twenty five which hasn't been revoked. So, you know, some intuitions: A one hundred has a peak performance of three hundred and twelve teraflop per second, and H one hundred has a peak performance of nineteen seventy-nine teraflop per second with sparsity and approximately fifty percent without. If you look at, you know, the NVIDIA has these specification sheets, so you can see that the flops actually depends on what you're trying to do. So if you're using FP32, it's actually really, really bad. Like if you run FP32 on H100, you're not getting its orders of magnitude worse than if you're doing FP16 or... and if you're. Willing to go down to FP eight, then it can be even faster. And you know, for the first read that I didn't realize, but there's an asterisk here, and this means with sparsity. So usually you're in a lot of the matrices we have in this class are dense, so you don't actually get this, you get something like, you know, half that number. EXACTLY HALF, okay. Okay，so。So now you can do a bunch of calculations, eight H one hundred for two weeks is just eight times the number of flops per second times the number of seconds in a week. Actually this might be one week, okay? So that's one week and that's four point seven times E to the twenty one. Which is you know some number, and you can kind of contextualize the flop counts with the other model counts. Yeah, so that means if... So what does sparsity mean? That means if your matrices are sparse. It's a specific like structured sparsity. It's like two out of four elements in each like group of four elements is zero. That's only case when you get that that speed. No one uses it. Yeah, it's a marketing department. Okay. So let's go through a simple example. So remember, we're not going to touch the transformer, but I think even a linear model gives us a lot of the building blocks and intuitions. So suppose we have endpoints, each point is d-dimensional, and the linear model is just going to map each d-dimensional vector to a k-dimensional vector. Okay? So let's set some number of points. Is B dimension is D, K is the number of outputs, and let's create our data matrix X, our weight matrix W, and the linear model is just some map mall, so nothing too interesting going on. you know, the question is how many flops was that? and the way you would look at this is you say, well, when you do the matrix multiplication, you have basically for every i j k triple, i have to multiply two numbers together. AND I ALSO HAVE TO ADD THAT NUMBER TO THE THE TOTAL, OKAY? SO THE ANSWER IS TWO TIMES, UM. BASICALLY THE PRODUCT OF ALL THE DIMENSIONS INVOLVED, SO THE LEFT DIMENSION, THE MIDDLE DIMENSION, AND THE RIGHT DIMENSION. Okay, so this is something that you should just kind of remember. If you're doing a matrix multiplication, the number of flops is two times the product of the three dimensions. Okay, so the flops of other operations are usually kind of linear in the size of the matrix or tensor, and in general no other operation you encounter deep learning as expensive as matrix multiplication for large enough matrices. So this is why I think a lot of the napkin math is very simple because we're only looking at the matrix multiplications that are performed by the model. Now, of course, there are regimes where if your matrices are small enough, then the cost of other things starts to dominate. But generally, that's not a good regime you want to be in because the hardware is designed for big matrix multiplication. So it's a little bit circular, but by kind of we end up in this regime where we only consider models where the mammals are the dominant. UH NO COST. Okay, any questions about this? This number two times the product of three dimensions. This is just a useful thing. Would the algorithm of metric modification always be the same because the chip might have optimized that? On these four links, the same. Yeah, so the question is like, does this essentially does this depend on the matrix multiplication algorithm? In general, I guess we'll look at this the next week when we or the week after when we look at kernels. I mean, actually there's a lot of optimization that goes underneath under the hood when it comes to matrix. Multiplications, and there's a lot of specialization depending on the shape. So this is, I would say, this is just a kind of a crude, you know, estimate that is basically like the right order of magnitude. Okay, so yeah, additions and multiplications are considered equivalent. So one way I find helpful to interpret this, so ultimately this is just a matrix multiplication, but I'm going to try to give a little bit of meaning to this, which is why I've set up this as kind of a little toy machine learning problem. So B is really stands for the number of data points. AND DK IS THE NUMBER OF PARAMETERS, SO FOR THIS PARTICULAR MODEL, THE NUMBER OF FLOPS THAT'S REQUIRED FOR FORWARD PASS IS TWO TIMES THE NUMBER OF TOKENS OR NUMBER OF DATA POINTS TIMES THE NUMBER OF PARAMETERS. Okay, so this turns out to actually generalize to transformers. There's an asterisk there because there's the sequence length and other stuff, but this is roughly right for if your sequence length isn't too large. UM SO. Okay, so now this is just a number of floating point operations, right? So how does this actually translate to wall clock time, which is presumably the thing you actually care about? How long do you have to wait for your run? So let's time this. So I have this function that is just going to. UM. DO IT FIVE TIMES, AND I'M GONNA PERFORM THE MATRIX MULTIPLY OPERATION. UM, WE'LL TALK A LITTLE BIT LATER ABOUT THIS. UM, TWO WEEKS FROM NOW, WHY THE OTHER CODE IS HERE? UM, BUT UM, FOR NOW WE GET AN ACTUAL TIME, SO THAT MATRIX... UH, TOOK YOU KNOW POINT ONE SIX SECONDS. UM, AND THE ACTUAL FLOPS PER SECOND, WHICH IS HOW MANY FLOPS DID IT DO PER SECOND, IS UM FIVE POINT FOUR E THIRTEEN. Okay, so now you can compare this with the marketing materials and for the A one hundred and H one hundred. And as we look at the spec sheet, the flops depends on the data type. And we see that the promise flops per second, which. you know for h one hundred for i guess this is for float thirty two is you know sixty seven you know terra flops as we looked and so that is the number of promise flops per second we we had. And now, if you look at the, there's a helpful notion called model flops utilization or MFU, which is the actual number of flops divided by the promise flops. Okay, so you take the actual number of flops, remember which is. What you actually witnessed, the number of floating point operations that are useful for your model divided by the actual time it took, divided by this promise float for a second, which is from the glossy brochure, you can get an MFU of .8. えっ？So usually you see people talking about their MFUs, and something greater than point five is you usually consider to be good, and if you're like five percent MFU, that's considered to be really bad. You usually can't get close to that close to ninety or one hundred because this is sort of ignoring all sort of communication and overhead. It's just like the literal. COMPUTATION OF A FLUMPS. Okay，and usually MFU is much higher if the matrix multiplications dominate。Okay，so that's MFU。Any questions about this？Yeah, you're using the promised block per sec, not considering sparsely. So this promise flock per sec is not considering this, and it's not today. One note is that this is actually a, you know, there's also something called hardware, you know, to flops utilization. And the motivation here is that we are trying to look at the. It's called model because we're looking at the number of effective useful operations that the model is performing, and so it's a way of kind of standardizing. It's not the actual number of flops that are done because you could have optimization in your code that cache a few things or redo recomputation of some things. And in some sense, you're still computing the same model. So what matters is that this is sort of trying to look at the model complexity, and you shouldn't be penalized just because you were clever in your MFU if you were clever and you didn't actually do the flops, but you said you did. Okay, so you can also do the same with BF sixteen. And here we see that for BF, the time is actually much better. Right, so point zero three instead of point one six. So the actual flops per second is higher. Even accounting for sparsity, the promise flops is still quite high. So the MFU X actually lower for BF sixteen. This is maybe surprisingly low, but sometimes the promise flops is a bit of optimistic. So always benchmark your code, and don't just kind of assume that you're going to get certain levels of performance. Okay, so just to summarize, matrix multiplications dominate the compute, and the general rule of thumb is that it's two times the product of the dimensions flops. The flops per second, floating points per second, depends on the hardware and also the data type. So the fancier the hardware you have, the higher it is. The smaller the data type, usually the faster it is. And MFU is a useful notion to look at how well you're essentially squeezing your hardware. Yeah, I bring it up and here to get like the maximum utilization. You want to use these like tensor cores on the machine, and so like this PyTorch by default use these tensor cores, and like are these calculations accounting for that? Yeah, so the question is what about those tensor cores? So if you go to this spec sheet. UM, YOU'LL SEE THAT, UM. These are all on the tensor core, so the tensor core is basically specialized hardware to do map malls. So if you are... if you're... so by default it should use it, and especially if you're using PyTorch compile, it will generate the code that will use the hardware. PROPERLY. Okay, so let's talk a little bit about gradients. And the reason is that we've only looked at matrix multiplication, or in other words, basically feed forward forward passes and the number of flops. But there's also a computation that comes from computing gradients, and we want to track down how much that is. Okay, so just to consider a simple example, a simple linear model where you take the prediction of a linear model and you look at the MSE with respect to five. So not a very interesting loss, but I think it's illustrative for looking at the gradients. Okay, so remember in the forward pass you have your X, you have your W, which you want to compute the gradient with respect to. You make a prediction by taking a linear product, and then you have your loss. Okay, and in the backward pass you just call loss backwards. In this case, the gradient, which is this variable attached to the tensor, turns out to be what you want. Okay, so everyone has done your gradients in PyTorch before. So let's look at how many flops are required for computing gradients. UM, okay, so. Let's look at a slightly more complicated model. So now it's a two-layer linear model where you have X, which is B by D times W one, which is D by D. So that's the first layer, and then you take your hidden activations H one and you pass it. Through another linear layer W two, and to get a K dimensional vector, and you do some compute some loss. Okay, so this is a two-layer linear network. And just as a kind of review, if you look at the number of forward flops, what you had to do was: you have to multiply. Look at W one, you have to multiply X by W one and add it to your H one. AND YOU HAVE TO TAKE H ONE AND W TWO, AND YOU HAVE TO ADD IT TO YOUR. Your H two, okay? So the total number of flops again is two times the product of all the dimensions in your map mall plus two times the product dimensions of your map mall for the second matrix. Okay? In other words, two times the total number of parameters in this case. Okay, so what about the backward pass? So this part will be a little bit more involved. So you can recall the model X to H one to H two and the loss. So in the backward pass, you have to compute a bunch of gradients. And the gradients that are relevant is you have to compute the gradient with respect to H one. H two W one and W two of the loss, so D loss D each of these variables. Okay, so how long does it take to compute that? Let's just look at W two for now. Okay, so the things that touch W two you can compute by looking at the chain rule. So W two grad, so the gradient of D loss D W two is you sum H one times the gradient of the loss with respect to H two. Okay, so that's just a chain rule for W two, and this is so all the gradients are the same size as the underlying vectors. So this turns out to be essentially looks like a matrix multiplication. And so the same calculus holds, which is that it's two times the number of the product of all the dimensions, B times D times K. Okay, but this is only the gradient with respect to W two. We also need to compute the gradient with respect to H one because we have to keep on back propagating to W one and so on. Okay, so that is going to be. UM, YOU KNOW, THE PRODUCT OF W TWO, UM, YOU KNOW, TIMES, UH, YOU KNOW, H TWO. Sorry, I think this should be the grad of H two H two grad. So that turns out to also be essentially looks like the matrix multiplication, and it's the same number of flops for computing the gradient of each one. Okay, so when you add the two, so that's just for W two, you do the same thing for W one, and that's which has D times D parameters. And when you add it all up, it's so for this, for W two, the amount of computation was four times B times D times K. AND FOR W ONE, IT'S ALSO FOUR TIMES B TIMES D TIMES D BECAUSE W ONE IS D BY D. OK. So I know there's a lot of symbols here, I'm going to try also to give you a visual account for this. So this is from a blog post that I think may work better. We'll see. Okay, I have to wait for the animation to loop back. So basically, this is one layer of the neural network where it has the hiddens and then the weights to the next layer. And so I have to okay, the problem with this animation is I have to wait okay, ready set okay. So first I have to multiply W and A, and I have to add it to this. That's a forward pass. And now I'm going to multiply this these two, and then add it to that. And I'm going to multiply and then add it to that okay. Any questions? In which the way to slow this down, but the details maybe I will let you kind of ruminate on. But the high level is that there is 2 times the number of parameters for the forward pass and 4 times the number of parameters for the backward pass, and we can just kind of work it out via the chain rule here. Yeah, for the homeworks, are we also using the you said some PyTorch implementation is allowed, some isn't? Are we allowed to use grad or we are doing the like entirely by hand doing the gradient? So the question is, in the homework, are you going to compute gradients by hand? And the answer is no, you're going to just use PyTorch gradient. This is just to break it down so we can do the counting flops. Okay，any questions about this before I move on？Okay, just to summarize, the forward pass is for this particular model is two times the number of data points times the number of parameters, and backwards is four times the number of data points times the number of parameters, which means that total it's six times number of data points times parameters. Okay, and that explains. Why there was at six in the beginning when I asked the motivating question? So now this is for a simple linear model. It turns out that many models, this is basically the bulk of a computation when essentially every computation you do has touches essentially a new parameters. ROUGHLY, AND YOU KNOW OBVIOUSLY THIS DOESN'T HOLD. YOU CAN FIND MODELS WHERE THIS DOESN'T HOLD BECAUSE YOU CAN HAVE LIKE ONE PARAMETER THROUGH PARAMETER SHARING, UM, AND HAVE YOU KNOW A BILLION FLOPS, BUT THAT'S GENERALLY WHAT NOT WHAT MODELS LOOK LIKE. Okay, so let me move on. So far, I've basically finished talking about the resource accounting. So we looked at tensors, we looked at some computation on tensors, we looked at how much tensors take to store, and also how many flops tensors take when you do various operations on them. Now let's start building up different models. I think this part isn't necessarily going to be that conceptually interesting or challenging, but it's more for maybe just completeness. Okay，so。So parameters in PyTorch are stored as these NN parameter objects. Let's talk a little bit about parameter initialization. So if you have, let's say, a. Your parameter that has okay, so you generate okay, sorry, your W parameter is an input dimension by hidden dimension matrix. You're still in the linear model case, so let's just turn it input and let's feed it through the output. Okay, so rand and unit or Gaussian is seems innocuous. What happens when you do this is that if you look at the output, you get some pretty large numbers, right? And this is because when you have the number grows as essentially the square root of the hidden dimension, and so when you have large models, this is going to blow up. And training can be very unstable, so typically what you want to do is initialize in a way that's invariant to hidden, or at least when you guarantee that it's not going to blow up. And one simple way to do this is just rescale by the one over square root number of inputs. So basically, let's redo this. W equals a parameter where I simply divide by the square root of the input dimension, and then now when you feed it through the output, now you get things that are stable around this. Actually concentrate to. you know something like normal zero one. Okay, so this is basically, you know, this has been explored pretty extensively, and deep learning literature is known up to a constant as savior initialization, and typically I guess it's fairly common. If you want to be extra safe, you don't trust the normal because it doesn't have it has unbounded tails, and you say I'm going to truncate to minus three three, so I don't get any large values, and I don't want any. TO MESS WITH THAT. OK. UM. Okay，so。UM, SO LET'S BUILD A YOU KNOW JUST A SIMPLE MODEL, UM. It's going to have D dimensions and two layers. There's this, you know, I just made up this name Cruncher. It's a custom model which is a deep linear network which has N num layers, layers, and each layer is a linear model which has essentially just a matrix multiplication. Okay, so this parameters of these this model is looks like I have. LAYERS FOR THE FIRST LAYER, WHICH IS A D BY D MATRIX; THE SECOND LAYER, WHICH IS ALSO A D BY D MATRIX; AND THEN I HAVE A HEAD OR A FINAL LAYER. OKAY? SO IF I GET THE NUMBER OF PARAMETERS OF THIS MODEL, THEN IT'S GOING TO BE D SQUARE PLUS D SQUARE PLUS D. Okay, so nothing too surprising there. And I'm going to move it to the GPU because I want this to run one fast. And I'm going to generate some random data and feed it through the data, and the forward pass is just going through the layers and then finally applying the head. Okay, so with that model, let's try to I'm going to use this model and do some stuff with it, but just one kind of general digression. Randomness is something that is sort of can be annoying in some cases if you're trying to reproduce a bug, for example. It shows up in many places, initialization, dropout, data ordering. UM, AND JUST THE BEST PRACTICES WE RECOMMEND YOU ALWAYS PASS A FIXED RANDOM SEED, UM, SO YOU CAN REPRODUCE YOUR YOUR MODEL OR AT LEAST AS WELL AS YOU CAN, UM. And in particular, having a different random seed for every source of randomness is nice because then you can, for example, fix initialization or fix the data ordering, but vary other things. Determinism is your friend when you're debugging and in code. Unfortunately, there's many places where you can use randomness and just be. Cognizant of which one you're using, and just if you want to be safe, just set the C two for all of them. Data loading, I guess I'll go through this quickly. It'll be useful for your assignment. So in language modeling, data is typically just a sequence of integers because this is remember output by the tokenizer and you serialize them into you can serialize them into numpy arrays. And one, I guess, thing that's maybe useful is that you don't want to load all your data into memory at once because, for example, the llama data is 2.8 terabytes, but you can sort of pretend to load it by using this handy function called memmap, which gives you essentially a variable that is mapped to a file, so when you try to access. The data, it actually on demand loads the file, and then using that you can create a data loader that is a samples data from your batch. So I am going to skip over that just in interest of time. UM, LET'S TALK A LITTLE BIT ABOUT YOUR OPTIMIZER. SO WE'VE DEFINED OUR MODEL. So there's many optimizers, just kind of maybe going through the intuitions behind some of them. So of course there's stochastic gradient descent: you compute the gradient of your batch, you take a step in that direction, no questions asked. There's an idea called momentum, which dates back to classic optimization Nesterov. Where you have a running average of your gradients, and you update against the running average instead of your instantaneous gradient. And then you have Adam, which you scale the gradients by your. UM, YOU'RE THE AVERAGE OVER THE THE NORMS OF YOUR OR I GUESS NOT THE NORMS, THE THE SQUARE OF THE GRADIENTS. YOU ALSO HAVE RMS PROP, WHICH IS A IMPROVED VERSION OF ADAGRAD, WHICH USES THE EXPONENTIAL AVERAGING RATHER THAN JUST LIKE A FLAT AVERAGE. AND THEN FINALLY ADAM, WHICH APPEARED IN TWENTY FOURTEEN, WHICH IS ESSENTIALLY COMBINING RMS PROP AND MOMENTUM. SO THAT'S WHY YOU'RE MAINTAINING BOTH. Your running average of your gradients, but also running average of your gradients squared. Okay, so since you're going to implement Adam in Homework One, I'm not going to do that. Instead, I'm going to implement you know Adam Grad. So the way you implement an optimizer. And PyTorch is that you override the optimizer class, and you have to... Let's see, maybe I'll... And I'll get to the implementation once we step through it. So let's define some data, compute the forward pass on the loss, and then. YOU COMPUTE THE GRADIENTS, AND THEN WHEN YOU CALL OPTIMIZER DOT STEP, THIS IS WHERE THE OPTIMIZER ACTUALLY IS ACTIVE. SO WHAT THIS LOOKS LIKE IS YOUR PARAMETERS ARE GROUPED BY, FOR EXAMPLE, YOU HAVE ONE FOR THE LAYER ZERO, LAYER ONE, AND THEN THE FINAL WEIGHTS. And you can access a state, which is a dictionary from parameters to whatever you want to store as optimizer state. The gradient of that parameter you assume is already calculated. By the backward pass, and now you can do things like you know in. In at a grad, you're storing the sum of the gradient squared, so you can get that G two variable, and you can update that based on the square of the gradient. So this is element wise squaring of the gradient, and you put it back into the state. Okay, so then your obviously your optimizer is responsible for updating the parameters, and this is just the you update the learning rate times the gradient divided by this scaling. So now this state is kept over across multiple invocations of the optimizer. Okay, so and then at the end of your optimizer step, you can free up the memory just to which is I think going to actually be more important when you look when we talk about model parallelism. Okay, so let's talk about the memory requirements of the optimizer states and actually basically at this point everything. So you need the number of parameters in this model is d squared times the number of layers plus d for the final head, okay? The number of activations, so this is something we didn't do before, but now for this simple model, it's fairly easy to do. It's just B times D times the number of layers you have for every layer, for every data point, for every dimension, you have to hold the activations. UM, FOR THE GRADIENTS, UM, THIS IS THE SAME AS THE NUMBER OF PARAMETERS AND THE NUMBER OF OPTIMIZER STATES. AND FOR ADAGRAD, IT'S UH... YOU REMEMBER WE HAD TO STORE UM. THE GRADIENT SQUARED, SO THAT'S ANOTHER COPY OF THE PARAMETERS. SO PUTTING IT ALL TOGETHER, WE HAVE UM THE TOTAL MEMORY IS ASSUMING YOU KNOW FP THIRTY-TWO, WHICH MEANS FOUR BYTES TIMES THE NUMBER OF PARAMETERS, NUMBER OF ACTIVATIONS. NUMBER OF GRADIENTS AND NUMBER OF OPTIMIZER STATES. Okay，and that gives us uh you know some number which. UM, IS FOUR HUNDRED AND NINETY SIX HERE. OKAY, SO THIS IS UM A FAIRLY SIMPLE CALCULATION IN THE ASSIGNMENT ONE. UM, YOU'RE GOING TO DO THIS FOR THE TRANSFORMER, WHICH IS A LITTLE BIT MORE INVOLVED BECAUSE YOU HAVE TO... THERE'S NOT JUST MATRIX MULTIPLICATIONS, BUT THERE'S MANY MATRICES, THERE'S ATTENTION, AND THERE'S ALL THESE OTHER THINGS. BUT THE GENERAL FORM OF THE CALCULATION IS THE SAME. YOU HAVE. PARAMETERS, ACTIVATIONS, GRADIENTS AND OPTIMIZER STATES. Okay, and the flops required again for this model is six times the number of tokens or the number of data points times the number of parameters, and that's basically concludes the resource locality for this particular model. UM, AND IF FOR REFERENCE, IF YOU ARE CURIOUS ABOUT WORKING THIS OUT FOR FOR TRANSFORMERS, YOU CAN CONSULT SOME OF THESE ARTICLES. Okay, so in the remaining time, I think maybe I'll pause for questions. We talked about building up the tensors, and then we built kind of a very small model, and we talked about optimization and how much memory and how much compute was required. Yeah. So the question is, why do you need to store the activations? So naively, you need to store the activations because when you're doing the backprop pass, the gradients of let's say the first layer depend on the activation. So the gradients of the ith layer depends on the activation there. UM, NOW IF YOU'RE SMARTER, YOU DON'T HAVE TO STORE THE ACTIVATIONS OR YOU DON'T HAVE TO STORE ALL OF THEM, YOU CAN RECOMPUTE THEM, AND THAT'S SOMETHING A TECHNICAL UH CALLED ACTIVATION CHECKPOINTING, WHICH WE CAN TALK ABOUT LATER. Okay, so let's just do this quick. Actually, there's not much to say here, but here's your typical training loop where you define the model, define the optimizer, and you get the data, feed forward, backward, and take a step in a parameter space. AND. UM, GUESS. It would be more interesting, I guess. Next time I should show like an actual one D plot, which isn't available on this version, but. So one note about checkpointing, so training language model takes a long time and you certainly will crash at some point, so you don't want to lose all your progress, so you want to periodically save your model to disk. And just to be very clear, the thing you want to save is both the model and the optimizer. And probably which iteration you're on, which add that, and then you can just load it up. One maybe final note and out, and is I alluded to kind of mixed precision training. Choice of the data type has different tradeoffs. If you have higher precision, it's more accurate. AND STABLE, UM, BUT IT'S MORE EXPENSIVE AND LOW PRECISIONS VICE VERSA, UM, AND AS WE MENTIONED BEFORE, BY DEFAULT THE RECOMMENDATIONS USE FLOAT THIRTY-TWO, UM, BUT TRY TO USE BF SIXTEEN OR EVEN FP-EIGHT WHENEVER POSSIBLE, SO YOU CAN USE LOWER PRECISION FOR THE FEET FORWARD PASS, UM, BUT FLOAT THIRTY-TWO FOR THE REST. And this is an idea that goes back to the 2017, there's exploring mixed precision training. PyTorch has some tools that automatically allow you to do mixed precision training because it can be sort of annoying to have to specify which parts of your model it needs to be what precision. Generally, you define your model as sort of this clean modular thing. And the specifying the precision is sort of like a, you know, something that needs to cut across that. And one, I guess maybe one kind of general comment is that people are pushing the envelope on what precision is needed. There's some papers that show you can actually use FP8 all the way through. I guess one of the challenges is, of course, when you have lower precision, it gets very numerically unstable. But then you can do various tricks to control the numerics of your model during training so that you don't get into these bad regimes. So this is where I think the systems and the model architecture design. Kind of are synergistic because you want to design models now that we have a lot of model design is just governed by hardware, so even the transformer as we mentioned last time is governed by having GPUs, and now if we notice that NVIDIA chips have the property that if lower precision, even like INT four for example, is one thing. Now, if you can make your model training actually work on in four, which is I think quite hard, then you can get massive speed ups and your model will be more efficient. There's another thing which we'll talk about later, which is often you'll train your model using more sane floating point, but when it comes to inference, you can go crazy and you take your preach model and then you can quantize it and get a lot of the gains from very, very aggressive quantization. So somehow training is a lot more difficult to do with low precision, but once you have a trained model, it's much easier to make it low precision. Okay, so I will wrap up there just to conclude. We have talked about the different primitives to use to train a model, building up from tensors all the way to the training loop. We talked about memory accounting and flops accounting for these simple models. Hopefully once you go through assignment one, all these concepts will be really solid because you'll be. Applying these ideas for the actual transformer. Okay, see you next time. 


=== NOTES ===
# Study Notes

## 1) Learning Goals
- Understand how tensors are created, stored, and moved between CPU and GPU in PyTorch.  
- Grasp the differences between floating‑point formats (FP32, FP16, BF16, FP8) and why they matter for memory and compute.  
- Learn how to count floating‑point operations (FLOPs) for matrix‑multiplication‑heavy models and connect this to real‑world GPU performance.  
- Explain the extra cost of back‑propagation and how it relates to the number of parameters.  
- Describe how Adam (and variants such as AdamW) keeps optimizer state and how that state affects memory.  
- Apply best practices for reproducibility, checkpointing, and mixed‑precision training in a training loop.

## 2) The Story in One Page

Training a large language model is a marathon that races through two major hurdles: memory and computation.  Every tensor – parameters, gradients, optimizer state – lives on a GPU and must fit within its high‑bandwidth memory.  At the same time, each forward and backward pass performs billions of math operations; the GPU’s raw FLOP rate determines how long these stalls last.  The teacher starts with the simplest building block, a PyTorch tensor, and traces its journey from CPU allocation to GPU residency.  He illustrates how the data type chosen (FP32 vs. BF16 vs. FP8) shapes both the footprint and the speed.  With a handful of equations, he explains how a matrix multiplication of shape (B, D) × (D, K) needs exactly `2 × B × D × K` FLOPs, and how the backward pass quadruples that amount because gradients must propagate through every parameter.  The story then moves to Adam, where an optimizer keeps not only gradients but also their squared averages, further expanding memory needs.  Finally, the instructor shows a concise training loop that emphasizes placing tensors on the GPU, seeding random generators for reproducibility, saving checkpoints, and enabling mixed‑precision via PyTorch tools.  By linking tensor memory, FLOPs, and optimizer state, students learn how to make a language‑model training pipeline sustainable on real hardware.

**Mini‑outline**

1. Tensors and memory basics  
2. Floating‑point formats and mixed precision  
3. Counting FLOPs for linear layers  
4. Gradient, back‑prop and extra cost  
5. Adam’s optimizer state and memory impact  
6. Practical training loop practices

## 3) Main Notes

### 3.1 Tensors & Memory
#### Overview
Tensors are the core data structures in PyTorch.  They hold multi‑dimensional arrays of numbers and come with metadata (shape, strides, data type).  A tensor can live on the CPU or on a GPU; the device must be specified explicitly or via `.to(device)`.  Understanding where a tensor resides is essential because memory resides in the GPU’s high‑bandwidth RAM and all computation must occur there.  Moving data back and forth from CPU to GPU incurs significant overhead.

#### Detailed Explanation
- **Shape & Elements**
  - A matrix `torch.randn(4,8)` creates 32 elements.
  - The memory cost = `num_elements × bytes_per_element`.
- **Data Types**
  - `float32` (FP32): 4 bytes, 32‑bit IEEE‑754 format.
  - `float16` (FP16): 2 bytes, half precision, halves memory but can underflow.
  - `bf16`: 2 bytes, same size as FP16 but with larger exponent field → retains much of FP32’s dynamic range.
  - `fp8`: 1 byte, very low precision, supported only on newer GPUs if needed.
- **Device Placement**
  - `torch.zeros(4,4).to('cuda')` puts the tensor directly on GPU memory.
  - `x.to('cuda')` copies CPU tensor `x` to GPU.
  - `torch.cuda.memory_allocated()` and `memory_reserved()` help check GPU usage.
- **Views & Contiguity**
  - Operations like `x[0]`, `x[:, 1]`, `x.t()` create *views* that share storage.
  - Non‑contiguous tensors may need `.contiguous()` before certain ops to guarantee efficient memory access.
  
#### Common Confusions
- **Misconception:** “A view is a copy.”  
- **Correction:** Views share the underlying storage; mutating the original tensor mutates the view and vice‑versa.

#### Check Yourself
- Create a tensor of shape (5,10) in FP16 on CPU.  Move it to GPU and compute the memory before and after the move.  
- Slice a tensor to create a view and verify both tensors share storage with `x.storage().data_ptr()`.  

### 3.2 Floating‑point Formats & Mixed Precision
#### Overview
Choosing a numeric format trades off memory, bandwidth, and numerical stability.  Modern GPUs expose tensor‑core kernels that are optimized for low‑precision arithmetic, enabling dramatic speedups when training large models.  However, training with low precision can cause underflow or overflow unless handled carefully.

#### Detailed Explanation
- **FP32 (Full Precision)**
  - Common default; stable but uses 4 GB per 1 B parameters (including gradients and optimizer state).
- **FP16 (Half Precision)**
  - Cuts memory and bandwidth in half.
  - Prone to overflow/underflow for very small / large values; often not suitable for training all layers.
- **BF16**
  - Same size as FP16 but 33‑bit exponent, offering dynamic range comparable to FP32.
  - Excellent for forward passes; still requires FP32 for accumulating gradients and optimizers.
- **FP8**
  - Introduced in 2022 by NVIDIA; only supported on the latest GPUs (e.g., H100).
  - Not generally recommended for training; may be used for inference or extreme compression.
- **Mixed Precision Training**
  - Use BF16 (or FP16) for the forward and some backward operations, but keep parameters and optimizer state in FP32.
  - PyTorch provides `torch.cuda.amp` and `torch.compile` to automate this.
  
#### Implementation Notes
- Enable autocast: `with torch.cuda.amp.autocast():` encloses the forward pass.  
- Scale gradients: `scaler.step(optimizer)` where `scaler = torch.cuda.amp.GradScaler()`.  
- Keep optimizer state FP32 by default; PyTorch does this automatically.

#### Common Confusions
- **Misconception:** “All operations will run in FP16 if I set the default dtype.”  
- **Correction:** Some ops (e.g., loss reduction, Adam) still expect FP32 to maintain stability.

#### Check Yourself
- Run a simple linear forward for a batch of 32 samples with `bf16` dtype.  Compare the time and memory usage against the same run with `fp32`.  
- Observe gradient norms for a tiny model; note if clipping is needed when using BF16.

### 3.3 Counting FLOPs for Linear Layers
#### Overview
In a neural network dominated by matrix multiplications, the number of floating point operations (FLOPs) can be estimated with a simple formula.  This estimate is the backbone of many resource calculations, including training time and memory bandwidth.

#### Detailed Explanation
- **Matrix Multiplication FLOPs**
  - For a multiplication `A @ B` where `A ∈ ℝ^(b×d)` and `B ∈ ℝ^(d×k)`,  
    FLOPs = `2 × b × d × k`.
  - The factor 2 accounts for one multiplication and one addition per output element.
- **Forward Pass Cost**
  - A linear layer with `D` input dims and `K` output dims has cost `2 × D × K` FLOPs per example.
  - For a batch of `B` examples: `2 × B × D × K`.
- **Total Compute for a Small Network**
  - Example: Two‑layer linear network with `D × D` matrices.  
    Forward cost = `2 B D²` + `2 B D K`.  
  - Backward cost doubles the forward cost for each layer due to parameter gradients, yielding `4 B D²` + `4 B D K`.
  - The entire training step needs `2 (number of parameters)` FLOPs for the forward pass plus `4 (number of parameters)` for backward, totaling `6 × number_of_parameters × B` FLOPs.

#### Common Confusions
- **Misconception:** “An addition counts as one FLOP, a multiplication as one FLOP.”  
- **Correction:** In this estimate, each multiplication **and** addition pair is counted as two FLOPs because they occur together in the inner loop.

#### Check Yourself
- Verify the FLOPs for a `torch.nn.Linear(128, 64)` layer on a single example (`B=1`).  
- Extend the calculation to a batch of `B=32`.  

### 3.4 Gradient, Back‑Prop and Extra Cost
#### Overview
Back‑propagation is not free: although gradients share the same shapes as parameters, the computational cost for computing them is **double** the forward pass for each layer plus an extra factor for intermediate activations that must be stored.

#### Detailed Explanation
- **Parameter Gradients**
  - For a weight matrix `W ∈ ℝ^(d×k)`, the gradient `∂L/∂W` is computed as `Hᵀ @ dL/dH` where `H` is the activation from the previous layer.  
  - This is also a matrix multiplication of shape `(k×d) @ (d×B)` → FLOPs `2 × B × d × k` (same as forward).
- **Activation Gradients**
  - Must propagate from later layers back to earlier ones.  Each backward matrix multiplication consumes as many FLOPs as the corresponding forward multiplication.
- **Total Backward Cost**
  - For a linear network:  
    - `W_1` gradient: `2 × B × D²`  
    - `W_2` gradient: `2 × B × D × K`  
    - Plus gradients for `H_1`: another `2 × B × D × K`  
    - Sum = `4 × B × D² + 4 × B × D × K`.
- **Relation to Parameters**
  - Counting all forward and backward FLOPs gives `6 × (number_of_parameters) × B`.

#### Common Confusions
- **Misconception:** “Backward pass costs the same as forward pass.”  
- **Correction:** Backward pass *doubles* the forward cost for each parameter matrix because each gradient requires a matrix multiplication.

#### Check Yourself
- For a single-layer linear with `D=64`, `K=32`, compute the theoretical FLOPs for forward and backward steps.  
- Confirm that the backward cost equals twice the forward cost for this layer.

### 3.5 Optimizer State & Memory
#### Overview
Optimizers such as Adam and AdamW maintain additional buffers per parameter (e.g., moving averages of gradients and squared gradients).  These buffers, together with activations and gradients, dominate the memory footprint of a training run.

#### Detailed Explanation
- **Parameter Count**
  - `num_params = sum over layers of (input_dim × output_dim) + output_dim_of_last_layer`.
- **Activations**
  - For a batch of `B` and a network of `L` layers with hidden size `D`,  
    Memory = `B × D × L × bytes_per_element`.
- **Gradients**
  - Same shape and datatype as parameters → add `num_params × bytes_per_element`.
- **Adam State (AdamW)**
  - Stores two buffers per parameter:  
    - First‑moment estimate `m` (mean of gradients).  
    - Second‑moment estimate `v` (mean of squared gradients).  
  - Each buffer is the same shape and datatype as the parameter, thus adding `2 × num_params × bytes_per_element`.
- **Total GPU Memory (FP32)**
  - `Memory = (num_params + 3 × num_params + activations) × 4 bytes`  
    (1 for the parameter, 1 for gradients, 2 for Adam state).
  - Example calculation from transcript: `470 GB` for a moderate model.

#### Implementation Notes
- PyTorch `optim.AdamW` keeps a `state` dictionary mapping each parameter to its `m` and `v` tensors.
- After each `optimizer.step()`, you can free temporary tensors with `torch.cuda.empty_cache()` if necessary.

#### Common Confusions
- **Misconception:** “Adam adds only a small constant to memory.”  
- **Correction:** Adam doubles the memory per parameter because of two extra buffers.

#### Check Yourself
- Instantiate a `torch.nn.Linear(1024, 1024)` and an AdamW optimizer.  Use `torch.cuda.memory_allocated()` to estimate the memory for parameters, gradients, and Adam state.  
- Verify that the recorded memory roughly matches `4 × (param_size + param_size + 2 × param_size)` bytes.

### 3.6 Training Loop & Practicalities
#### Overview
A robust training loop combines correct device placement, reproducibility, checkpointing, and mixed‑precision handling.  These practices prevent frustrating crashes, ease debugging, and keep training efficient.

#### Detailed Explanation
- **Reproducibility**
  - Set a fixed random seed for every source of randomness:  
    ```python
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    np.random.seed(42)
    random.seed(42)
    ```
  - Use a separate seed for data shuffling vs. model initialization if differential control is desired.
- **Checkpointing**
  - Save a checkpoint every `N` iterations or epochs:  
    ```python
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epoch': epoch,
        'loss': loss.item(),
    }, f'ckpt_{epoch}.pt')
    ```
  - `torch.load` can restore both model and optimizer to resume training.
- **Memory‑Efficient Data Loading**
  - For massive corpora (e.g., LLaMA 2.8 TB), use `memmap` to load data on demand:
    ```python
    data = np.memmap('tokens.npy', dtype='int32', mode='r')
    ```
  - Build a `torch.utils.data.DataLoader` that samples from the memmap array.
- **Mixed‑Precision Training**
  - Wrap the forward pass in `torch.cuda.amp.autocast()`.  
  - Use `torch.cuda.amp.GradScaler` to scale gradients and call `scaler.step(optimizer)`.  
  - Keep the module parameters in FP32; only forward ops are casted.
- **Training Loop Skeleton**
  ```python
  model.to('cuda')
  optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
  scaler = torch.cuda.amp.GradScaler()
  
  for epoch in range(num_epochs):
      for batch in dataloader:
          inputs, targets = batch[0].to('cuda'), batch[1].to('cuda')
          optimizer.zero_grad()
          with torch.cuda.amp.autocast():
              outputs = model(inputs)
              loss = loss_fn(outputs, targets)
          scaler.scale(loss).backward()
          scaler.step(optimizer)
          scaler.update()
  ```

#### Common Confusions
- **Misconception:** “`model.to('cuda')` moves all tensors automatically.”  
- **Correction:** Only the parameters are moved; all other tensors created during the forward pass must be explicitly moved or passed with `.to('cuda')`.

#### Check Yourself
- Write a minimal training loop that trains a two‑layer linear network on synthetic data using mixed precision.  
- Verify that the number of GPUs used is one and that the training time per epoch is roughly halved compared to a single‑precision run.

---
